
def conv1_bias():
    scale_in=0.007843137718737125
    scale_w=0.009006182663142681
    bias=[ 0.0615, -1.1525, -0.6793, -1.5206, -0.5132]
    bias_int32 = [round(x/(scale_in*scale_w)) for x in bias]
    print(bias_int32)

def conv1_scale():
    scale_in=0.007843137718737125
    scale_w=0.009006182663142681
    scale_out=0.03605441376566887
    scale_int32 = round(scale_out/(scale_in*scale_w))
    print(scale_int32)

def conv2_bias():
    scale_in=0.03605441376566887
    scale_w=0.008814302273094654
    bias=[-0.0682,  0.6372, -0.7284,  1.3115,  0.1402]
    bias_int32 = [round(x/(scale_in*scale_w)) for x in bias]
    print(bias_int32)

def conv2_scale():
    scale_in=0.03605441376566887
    scale_w=0.008814302273094654
    scale_out=0.06869560480117798
    scale_int32 = round(scale_out/(scale_in*scale_w))
    print(scale_int32)

def fc1_bias():
    scale_in=0.06869560480117798
    scale_w=0.009187890216708183
    bias=[-0.2007, -0.2739,  1.0864, -0.1819, -0.5550,  0.5506, -0.4155, -0.4199,
         1.4843, -1.4339]
    bias_int32 = [round(x/(scale_in*scale_w)) for x in bias]
    print(bias_int32)

def fc1_scale():
    scale_in=0.06869560480117798
    scale_w=0.009187890216708183
    scale_out=0.13029447197914124
    scale_int32 = round(scale_out/(scale_in*scale_w))
    print(scale_int32)

def fc2_bias():
    scale_in=0.13029447197914124
    scale_w=0.13029447197914124
    bias=[-0.0781,  1.4937, -1.2087, -0.2365, -0.0269, -0.5396,  0.3720,  0.4710,
        -1.0915,  0.3149]
    bias_int32 = [round(x/(scale_in*scale_w)) for x in bias]
    print(bias_int32)

def fc2_scale():
    scale_in=0.13029447197914124
    scale_w=0.13029447197914124
    scale_out=0.18027548491954803
    scale_int32 = round(scale_out/(scale_in*scale_w))
    print(scale_int32)


# conv1_bias()
# conv1_scale()
# conv2_bias()
# conv2_scale()
# fc1_bias()
# fc1_scale()
fc2_bias()
fc2_scale()


"""
Using device: cuda
Epoch [1/20], Loss: 0.7151, Train Accuracy: 76.04%
Epoch [2/20], Loss: 0.2386, Train Accuracy: 92.49%
Epoch [3/20], Loss: 0.2025, Train Accuracy: 93.70%
Epoch [4/20], Loss: 0.1843, Train Accuracy: 94.29%
Epoch [5/20], Loss: 0.1722, Train Accuracy: 94.62%
Epoch [6/20], Loss: 0.1645, Train Accuracy: 94.91%
Epoch [7/20], Loss: 0.1586, Train Accuracy: 95.00%
Epoch [8/20], Loss: 0.1560, Train Accuracy: 95.12%
Epoch [9/20], Loss: 0.1528, Train Accuracy: 95.32%
Epoch [10/20], Loss: 0.1494, Train Accuracy: 95.38%
Epoch [11/20], Loss: 0.1478, Train Accuracy: 95.37%
Epoch [12/20], Loss: 0.1469, Train Accuracy: 95.41%
Epoch [13/20], Loss: 0.1434, Train Accuracy: 95.44%
Epoch [14/20], Loss: 0.1422, Train Accuracy: 95.62%
Epoch [15/20], Loss: 0.1419, Train Accuracy: 95.54%
Epoch [16/20], Loss: 0.1405, Train Accuracy: 95.64%
Epoch [17/20], Loss: 0.1413, Train Accuracy: 95.58%
Epoch [18/20], Loss: 0.1366, Train Accuracy: 95.66%
Epoch [19/20], Loss: 0.1354, Train Accuracy: 95.81%
Epoch [20/20], Loss: 0.1371, Train Accuracy: 95.80%
FP32 Model Test Accuracy: 95.95%
===== Debug Inference Start =====
>>> 原始输入图像 (FP32) shape: torch.Size([1, 1, 28, 28])
tensor([[[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3412,
            0.4510,  0.2471,  0.1843, -0.5294, -0.7176, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.7412,
            0.9922,  0.9922,  0.9922,  0.9922,  0.8902,  0.5529,  0.5529,
            0.5529,  0.5529,  0.5529,  0.5529,  0.5529,  0.5529,  0.3333,
           -0.5922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4745,
           -0.1059, -0.4353, -0.1059,  0.2784,  0.7804,  0.9922,  0.7647,
            0.9922,  0.9922,  0.9922,  0.9608,  0.7961,  0.9922,  0.9922,
            0.0980, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -0.8667, -0.4824, -0.8902,
           -0.4745, -0.4745, -0.4745, -0.5373, -0.8353,  0.8510,  0.9922,
           -0.1686, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -0.3490,  0.9843,  0.6392,
           -0.8588, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -0.8275,  0.8275,  1.0000, -0.3490,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000,  0.0118,  0.9922,  0.8667, -0.6549,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -0.5373,  0.9529,  0.9922, -0.5137, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000,  0.0431,  0.9922,  0.4667, -0.9608, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -0.9294,  0.6078,  0.9451, -0.5451, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -0.0118,  0.9922,  0.4275, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -0.4118,  0.9686,  0.8824, -0.5529, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8510,
            0.7333,  0.9922,  0.3020, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9765,  0.5922,
            0.9922,  0.7176, -0.7255, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7020,  0.9922,
            0.9922, -0.3961, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -0.7569,  0.7569,  0.9922,
           -0.0980, -0.9922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000,  0.0431,  0.9922,  0.9922,
           -0.5922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -0.5216,  0.8980,  0.9922,  0.9922,
           -0.5922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -0.0510,  0.9922,  0.9922,  0.7176,
           -0.6863, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -0.0510,  0.9922,  0.6235, -0.8588,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],
          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,
           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]]])

>>> 输入量化 (QuantStub)
QuantStub scale: 0.007843137718737125
QuantStub zero_point: 127
Quantized input int_repr:
 tensor([[[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,  84, 185, 159, 151,  59,  35,   0,   0,
             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0, 221, 253, 253, 253, 253, 240, 197, 197,
           197, 197, 197, 197, 197, 197, 170,  51,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,  67, 114,  72, 114, 163, 226, 253, 224,
           253, 253, 253, 249, 228, 253, 253, 140,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  17,  66,  14,
            67,  67,  67,  59,  21, 235, 253, 106,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             0,   0,   0,   0,  83, 252, 209,  18,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             0,   0,   0,  22, 232, 254,  83,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             0,   0,   0, 129, 253, 237,  43,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             0,   0,  59, 248, 253,  61,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             0,   0, 133, 253, 187,   5,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             0,   9, 205, 247,  57,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             0, 126, 253, 181,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
            75, 250, 239,  57,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  19,
           221, 253, 166,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3, 203,
           253, 219,  35,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  37, 253,
           253,  77,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  31, 223, 253,
           115,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 133, 253, 253,
            51,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  61, 241, 253, 253,
            51,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 253, 253, 219,
            39,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 253, 207,  18,
             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]]],
       dtype=torch.uint8)

>>> 第一次 MaxPool2d 后 int_repr shape: torch.Size([1, 1, 14, 14])

>>> Conv1 权值相关信息:
  量化模式(qscheme): torch.per_tensor_affine
  weight scale: 0.009006182663142681
  weight zero_point: 2
  weight int_repr: tensor([  53,   51,   60,  -35,  -20,   27,  -26,  -31,  -10,   -1,   55,    2,      
         -56,  127,  -39,   31,  107,  -72,  -75,  -65,   33,   57,  -67,  -57,
          53,   57,   16,   43,   16,  -44,   10, -115,  -20, -128,  -14,   75,
           2,  -70,  -11,   46,  106,  105,   15,  -34,  -24],
       dtype=torch.int8)
  bias (float): <bound method Conv2d.bias of QuantizedConv2d(1, 5, kernel_size=(3, 3), stride=(1, 1), scale=0.03605441376566887, zero_point=159)>
Bias value: Parameter containing:
tensor([ 0.0615, -1.1525, -0.6793, -1.5206, -0.5132], requires_grad=True)

>>> Conv1 输出:
  out1 scale:      0.03605441376566887
  out1 zero_point: 159
  out1 int_repr shape: torch.Size([1, 5, 12, 12])
  out1 int_repr (前10个元素):
 tensor([148, 148, 148, 148, 148, 148, 148, 148, 148, 148], dtype=torch.uint8)
>>> ReLU 后 int_repr (前10个元素):
 tensor([159, 159, 159, 159, 159, 159, 159, 159, 159, 159], dtype=torch.uint8)
>>> MaxPool2d 后 int_repr shape: torch.Size([1, 5, 6, 6])

>>> Conv2 权值相关信息:
  量化模式(qscheme): torch.per_tensor_affine
  weight scale: 0.008814302273094654
  weight zero_point: 31
  weight int_repr: tensor([  34,   50,   78,  104,   54,  -51,   38,   37,  -22,   44,   31,   26,
          42,   96,   62,   76,   92,   73,   -9,   52,   85,    7,   38,   65,
          64,    4,    9,    5,  -28,    7,  -31,  -28,    4,   62,   38,   18,
          53,   44,  -17,   35,  -17,   13,   78,   11,   39,   26,  -44,   12,
          29,  -65,   67,   62,   86,   29,   60,   90,   50,   87,  101,   38,
         109,   74,   52,   49,   -6,   21,  -19,  -51,   50,  -57,   74,   53,
          59,   80,   61,    2,  -15,   34,  -62, -128,  -35,  -38,   54,   76,
          41,   33,   85,  -44,   22,  112,   16,   47,   35,   89,  127,   98,
          12,   39,   27,   -3,    7,   -5,   32,    0,   38,   31,   35,   34,
          15,   59,   29,   51,   23,   36,   39,   23,   35,  -19,   11,   15,
          21,   17,   42,   67,   56,   38,   63,   78,   56,   44,   95,   52,
           2,   62,   17,  -33,  -15,   46,   -3,   -4,   97,   80,   55,   43,
          -6,   24,   56,  -13,   -1,   21,  -14,  -35,  -33,   47,   45,   21,
          52,   69,   66,  -14,   69,   85,   27,   24,   26,   36,   40,   -1,
          24,   30,   14,   22,   20,   46,   19,   24,   51,   18,   35,   76,
          39,   29,   66,   29,  -11,  -45,   48,  -32,   53,   -9,  -26,  -50,
          50,  112,   88,  -14,  -10,   -2,   24,   97,  122,  -44,   58,   20,
          14,  -50,   -8,   25,   11,   26,   12,   13,  -23,  -18,   64,   86,
          92,   32,   33,   77,   48,   12,  124,   72,   19],
       dtype=torch.int8)
  bias (float): <bound method Conv2d.bias of QuantizedConv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), scale=0.06869560480117798, zero_point=118)>
Bias value: Parameter containing:
tensor([-0.0682,  0.6372, -0.7284,  1.3115,  0.1402], requires_grad=True)

>>> Conv2 输出:
  out2 scale:      0.06869560480117798
  out2 zero_point: 118
  out2 int_repr shape: torch.Size([1, 5, 4, 4])
  out2 int_repr (前10个元素):
 tensor([108, 101, 118, 145, 110, 136, 162, 177, 127, 140], dtype=torch.uint8)
>>> ReLU+MaxPool2d 后 shape: torch.Size([1, 5, 2, 2])

>>> Flatten 后 shape: torch.Size([1, 20])
  out3 int_repr (前10个元素):
 tensor([136, 177, 140, 141, 118, 118, 133, 167, 182, 178], dtype=torch.uint8)

>>> FC1 权值相关信息:
  量化模式(qscheme): torch.per_tensor_affine
  weight scale: 0.009187890216708183
  weight zero_point: 7
  weight int_repr: tensor([  -4,  -38,   10,   64,   71,   16,  -49,   31,    2,   73,   68,    7,      
          92,  -14,    8, -103,   86,  -19,   55,  -84,   81,  -56,  -20,   78,
         -82,    2,  -55,   33,   85,  -18,  -51,   62,   61, -110,   86,  -12,
           2,  127,   79,   86,   26,  -31,   36,   14,   -6,   38,   41,   13,
         -60,   23,  -37,   38,   63,  -67,   89,  116,   52,  -20,   83,   34,
          10,    4,  -10,   -6,   26,   -6,   22,  -14,   13,  -14,    7,    5,
          -1,    3,  -10,   -1,   10,   11,   18,   28,   48,   34,   13,   -7,
         103,  -11,   98,   58,  -43,  -52,   34,   11,   40,  -26,  -19,    1,
         -26,   92,   54,  -24,  -29,   50,   36,   15,    8,   40,   80,   18,
         -54,   -3,  -16,  -85,   -1,   49,  -30,  -32,   28,   47,   -2,  -77,
          95, -128,   34,   31,   39,   52,   51,   80,  -32,   89,  -37,   74,
          99,    0,  -59,  -40,  -14,   -2,  -12,   98,   65,   52,  -80,  -87,
        -110,   12,   43,  114,   59,  -21,   21,  110,   34,  -35,  -36,   72,
          19,  -17,   79,   38,   49,   15,   51,  -33,  -23,   25,   10,   86,
          58,   46,  -75,   39,  107,  -18,  -35,   -5,   14,   55,   13,  -25,
         -28,   31,    6,   56,   56,  -22,   40,  -26,    7,  119,  -38,  -30,
          14,    4,   27,   32,   -3,   56,   76,   45], dtype=torch.int8)
  bias (float): <bound method Linear.bias of QuantizedLinear(in_features=20, out_features=10, scale=0.13029447197914124, zero_point=107, qscheme=torch.per_tensor_affine)>
Bias value: Parameter containing:
tensor([-0.2007, -0.2739,  1.0864, -0.1819, -0.5550,  0.5506, -0.4155, -0.4199,
         1.4843, -1.4339], requires_grad=True)

>>> FC1 输出:
  out_fc1 scale:      0.13029447197914124
  out_fc1 zero_point: 107
  out_fc1 int_repr: tensor([[143, 131, 107,  90, 112, 121, 132, 135, 202, 144]], dtype=torch.uint8)     

>>> FC2 权值相关信息:
  量化模式(qscheme): torch.per_tensor_affine
  weight scale: 0.010628371499478817
  weight zero_point: 11
  weight int_repr: tensor([ -56,  -80,   38,  -11,   14,   57,  -47,   54,   -3,  101,  -46,   61,      
          90,   15,  -35,   99,  -46,  -55,  109, -122,   -8,   20,   15,  -15,
          30,  -67,  -41,   90,   51,   28,   -5,  127,   -7,  -17,  -27,    3,
         -25,  -15,    0,   23,   87,  -22,   13,  -15,   99,  -45,    7,   13,
          -9, -128,   48,   22,   41,   11,  -67,   13,   42,  -41,  -49,   74,
         -88,  -52,   40,   -2,   54,   -8,   88,   10,  -68,   22,   47,    3,
         -96,   37,  -48,   50,    1,   50,  119,  -13,  -23,   54,  -11,   15,
          48,  -37,   56,  -37,   51,   34,   84,   13,  -82,   38,   62,   -1,
         -33,    1,    3,   26], dtype=torch.int8)
  bias (float): <bound method Linear.bias of QuantizedLinear(in_features=10, out_features=10, scale=0.18027548491954803, zero_point=130, qscheme=torch.per_tensor_affine)>
Bias value: Parameter containing:
tensor([-0.0781,  1.4937, -1.2087, -0.2365, -0.0269, -0.5396,  0.3720,  0.4710,
        -1.0915,  0.3149], requires_grad=True)

>>> FC2 输出 (Logits):
  out_fc2 scale:      0.18027548491954803
  out_fc2 zero_point: 130
  out_fc2 int_repr: tensor([[113, 148, 153, 126,  88, 105,  53, 221, 153, 141]], dtype=torch.uint8)     

>>> 最终输出 (反量化后) => logits:
tensor([[ -3.0647,   3.2450,   4.1463,  -0.7211,  -7.5716,  -4.5069, -13.8812,
          16.4051,   4.1463,   1.9830]])
===== Debug Inference End =====

推理结果（预测类别）: 7
真实类别: 7
"""